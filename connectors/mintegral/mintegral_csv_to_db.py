import pandas as pd
from sqlalchemy import create_engine, text
import logging
import os
import glob
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

HOST = 'your_host_here'
PORT = 3306  # –ü–æ—Ä—Ç MariaDB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
USER = 'your_username_here'
PASSWORD = 'your_password_here'
DATABASE = 'your_database_name_here'
TABLE = 'mintegral_api_data'


class DatabaseManager:
    def __init__(self):
        self.connection_string = f'mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4'
        self.engine = create_engine(self.connection_string, pool_recycle=3600, pool_pre_ping=True, echo=False)

    def test_connection(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.engine.begin() as connection:
                connection.execute(text("SELECT 1"))
            logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def create_database_if_not_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        try:
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            engine_no_db = create_engine(f'mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/')

            with engine_no_db.begin() as connection:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                result = connection.execute(text(f"SHOW DATABASES LIKE '{DATABASE}'"))
                if not result.fetchone():
                    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                    connection.execute(
                        text(f"CREATE DATABASE {DATABASE} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"))
                    logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {DATABASE} —Å–æ–∑–¥–∞–Ω–∞")
                else:
                    logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {DATABASE} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

            engine_no_db.dispose()
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def create_table_if_not_exists(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –ò–°–ü–†–ê–í–õ–ï–ù–û –¥–ª—è MariaDB"""
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {TABLE} (
            account_id INT NOT NULL,
            account_name TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
            date DATE NOT NULL,
            campaign_name TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
            impression INT DEFAULT 0,
            clicks INT DEFAULT 0,
            spend_in_dollars DECIMAL(15,4) DEFAULT 0.0000,
            INDEX idx_account_date (account_id, date),
            INDEX idx_date (date),
            INDEX idx_account_id (account_id),
            INDEX idx_campaign_hash (account_id, date, campaign_name(255))
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """

        try:
            with self.engine.begin() as connection:
                connection.execute(text(create_table_sql))
            logger.info(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {TABLE} –≥–æ—Ç–æ–≤–∞")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã: {e}")
            return False

    def get_existing_records_count(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        try:
            with self.engine.begin() as connection:
                result = connection.execute(text(f"SELECT COUNT(*) FROM {TABLE}"))
                count = result.fetchone()[0]
                return count
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π: {e}")
            return 0

    def remove_duplicates_before_insert(self, df):
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π –≤ –ë–î"""
        if df.empty:
            return df

        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ account_id + date + campaign_name –∏–∑ –ë–î
            existing_query = f"""
                SELECT DISTINCT account_id, date, campaign_name 
                FROM {TABLE}
            """

            with self.engine.begin() as connection:
                existing_df = pd.read_sql(existing_query, connection)

            if existing_df.empty:
                logger.info("–ë–î –ø—É—Å—Ç–∞, –≤—Å–µ –∑–∞–ø–∏—Å–∏ –±—É–¥—É—Ç –Ω–æ–≤—ã–º–∏")
                return df

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            before_count = len(df)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            merged = df.merge(
                existing_df,
                on=['account_id', 'date', 'campaign_name'],
                how='left',
                indicator=True
            )

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ (–∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ë–î)
            df_clean = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)

            after_count = len(df_clean)
            duplicates_found = before_count - after_count

            if duplicates_found > 0:
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –∏ —É–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_found}")
                logger.info(f"üìä –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {after_count}")
            else:
                logger.info(f"‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤—Å–µ {after_count} –∑–∞–ø–∏—Å–µ–π –Ω–æ–≤—ã–µ")

            return df_clean

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            logger.warning("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            return df

    def save_dataframe(self, df):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å DataFrame –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if df.empty:
            logger.warning("DataFrame –ø—É—Å—Ç, –Ω–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å")
            return False

        try:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π
            df_clean = self.remove_duplicates_before_insert(df)

            if df_clean.empty:
                logger.info("–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
                return True

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            df_clean.to_sql(
                name=TABLE,
                con=self.engine,
                if_exists='append',
                index=False,
                chunksize=5000,
                method='multi'
            )

            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {len(df_clean)}")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def get_data_summary(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –¥–∞–Ω–Ω—ã–º –≤ –ë–î"""
        try:
            query = f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT campaign_name) as unique_campaigns,
                    COUNT(DISTINCT account_id) as unique_accounts,
                    MIN(date) as min_date,
                    MAX(date) as max_date,
                    SUM(impression) as total_impressions,
                    SUM(clicks) as total_clicks,
                    SUM(spend_in_dollars) as total_spend
                FROM {TABLE}
            """

            with self.engine.begin() as connection:
                result = connection.execute(text(query))
                row = result.fetchone()

                if row:
                    return {
                        'total_records': row[0],
                        'unique_campaigns': row[1],
                        'unique_accounts': row[2],
                        'min_date': row[3].strftime('%Y-%m-%d') if row[3] else None,
                        'max_date': row[4].strftime('%Y-%m-%d') if row[4] else None,
                        'total_impressions': row[5] or 0,
                        'total_clicks': row[6] or 0,
                        'total_spend': float(row[7]) if row[7] else 0
                    }
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")
            return None

    def optimize_table(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã"""
        try:
            with self.engine.begin() as connection:
                connection.execute(text(f"OPTIMIZE TABLE {TABLE}"))
            logger.info("‚úÖ –¢–∞–±–ª–∏—Ü–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")


def find_csv_files():
    """–ù–∞–π—Ç–∏ —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π CSV —Ñ–∞–π–ª –æ—Ç –Ω–∞—à–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞"""
    # –ò—â–µ–º —Ñ–∞–π–ª—ã –Ω–∞—à–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É mintegral_data_YYYYMMDD_HHMMSS.csv
    mintegral_files = glob.glob('mintegral_data_*.csv')

    if not mintegral_files:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã mintegral_data_*.csv, –∏—â–µ–º –ª—é–±—ã–µ CSV —Ñ–∞–π–ª—ã")
        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∞—à–∏—Ö —Ñ–∞–π–ª–æ–≤, –±–µ—Ä–µ–º –ª—é–±—ã–µ CSV
        csv_files = glob.glob('*.csv')
        if csv_files:
            return [max(csv_files, key=os.path.getctime)]  # –°–∞–º—ã–π –Ω–æ–≤—ã–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
        else:
            return []

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ —Ñ–∞–π–ª—ã –ø–æ timestamp –≤ –∏–º–µ–Ω–∏ (—Å–∞–º—ã–π –Ω–æ–≤—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–º)
    def extract_timestamp(filename):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç timestamp –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ mintegral_data_YYYYMMDD_HHMMSS.csv"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç—å YYYYMMDD_HHMMSS –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            basename = os.path.basename(filename)
            timestamp_part = basename.replace('mintegral_data_', '').replace('.csv', '')
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            return datetime.strptime(timestamp_part, '%Y%m%d_%H%M%S')
        except:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–µ–Ω—å —Å—Ç–∞—Ä—É—é –¥–∞—Ç—É
            return datetime(1900, 1, 1)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ timestamp –∏ –±–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π
    sorted_files = sorted(mintegral_files, key=extract_timestamp, reverse=True)
    latest_file = sorted_files[0]

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ mintegral_data: {len(mintegral_files)}")
    logger.info(f"–í—ã–±—Ä–∞–Ω —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª: {latest_file}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª
    return [latest_file]


def load_csv_file(filename):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å CSV —Ñ–∞–π–ª"""
    try:
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª: {filename}")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        encodings = ['utf-8', 'cp1251', 'windows-1251']

        for encoding in encodings:
            try:
                df = pd.read_csv(filename, encoding=encoding)
                logger.info(f"–§–∞–π–ª {filename} –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")
                logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
                return df
            except UnicodeDecodeError:
                continue

        # –ï—Å–ª–∏ –≤—Å–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –Ω–µ –ø–æ–¥–æ—à–ª–∏
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª {filename} –Ω–∏ —Å –æ–¥–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π")
        return pd.DataFrame()

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
        return pd.DataFrame()


def validate_csv_structure(df):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É CSV —Ñ–∞–π–ª–∞"""
    required_columns = [
        'account_id', 'account_name', 'date', 'campaign_name',
        'impression', 'clicks', 'spend_in_dollars'
    ]

    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        logger.error(f"–í CSV —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        return False

    logger.info("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ CSV —Ñ–∞–π–ª–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
    return True


def parse_date_column(date_str):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    if pd.isna(date_str) or date_str == '':
        return None

    # –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    date_formats = [
        '%Y-%m-%d',  # 2025-01-01 (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –∏–∑ —ç–∫—Å–ø–æ—Ä—Ç–∞)
        '%d.%m.%Y',  # 01.01.2025
        '%d/%m/%Y',  # 01/01/2025
        '%Y/%m/%d',  # 2025/01/01
        '%d-%m-%Y',  # 01-01-2025
    ]

    for fmt in date_formats:
        try:
            return datetime.strptime(str(date_str), fmt).date()
        except ValueError:
            continue

    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}")
    return None


def prepare_dataframe_for_db(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å DataFrame –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î"""
    if df.empty:
        return df

    logger.info("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é DataFrame
    df_clean = df.copy()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—ã
    df_clean['date'] = df_clean['date'].apply(parse_date_column)

    # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
    before_count = len(df_clean)
    df_clean = df_clean.dropna(subset=['date'])
    after_count = len(df_clean)

    if before_count != after_count:
        logger.warning(f"–£–¥–∞–ª–µ–Ω–æ {before_count - after_count} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏")

    # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    df_clean['account_id'] = pd.to_numeric(df_clean['account_id'], errors='coerce').fillna(0).astype(int)
    df_clean['impression'] = pd.to_numeric(df_clean['impression'], errors='coerce').fillna(0).astype(int)
    df_clean['clicks'] = pd.to_numeric(df_clean['clicks'], errors='coerce').fillna(0).astype(int)
    df_clean['spend_in_dollars'] = pd.to_numeric(df_clean['spend_in_dollars'], errors='coerce').fillna(0).round(4)

    # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –∫ —Å—Ç—Ä–æ–∫–∞–º
    df_clean['account_name'] = df_clean['account_name'].astype(str)
    df_clean['campaign_name'] = df_clean['campaign_name'].astype(str)

    # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ –∑–∞–ø–∏—Å–∏
    df_clean = df_clean.dropna(subset=['campaign_name', 'account_name'])

    logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(df_clean)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
    return df_clean


def main():
    print("MINTEGRAL CSV TO DATABASE LOADER")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
    if not all([HOST, USER, PASSWORD, DATABASE]):
        logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î!")
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ë–î
    db_manager = DatabaseManager()

    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not db_manager.create_database_if_not_exists():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
        return

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    if not db_manager.test_connection():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    if not db_manager.create_table_if_not_exists():
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É")
        return

    # –ü–æ–∏—Å–∫ CSV —Ñ–∞–π–ª–æ–≤ (—Ç–æ–ª—å–∫–æ —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
    csv_files = find_csv_files()

    if not csv_files:
        logger.error("CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        logger.info("–û–∂–∏–¥–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã: mintegral_data_YYYYMMDD_HHMMSS.csv")
        return

    logger.info(f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {csv_files[0]}")

    # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏
    summary_before = db_manager.get_data_summary()
    if summary_before:
        logger.info(f"–ó–∞–ø–∏—Å–µ–π –≤ –ë–î –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏: {summary_before['total_records']}")
    else:
        logger.info("–ë–î –ø—É—Å—Ç–∞")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–∞ (—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω - —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
    csv_file = csv_files[0]

    # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
    df = load_csv_file(csv_file)

    if df.empty:
        logger.error(f"–§–∞–π–ª {csv_file} –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    if not validate_csv_structure(df):
        logger.error(f"–§–∞–π–ª {csv_file} –∏–º–µ–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
        return

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df_prepared = prepare_dataframe_for_db(df)

    if df_prepared.empty:
        logger.warning(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª {csv_file} –æ–∫–∞–∑–∞–ª—Å—è –ø—É—Å—Ç")
        return

    logger.info(f"–§–∞–π–ª {csv_file} –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {len(df_prepared)} –∑–∞–ø–∏—Å–µ–π")

    # –£–¥–∞–ª—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–∞–º–æ–º —Ñ–∞–π–ª–µ
    before_dedup = len(df_prepared)
    df_prepared = df_prepared.drop_duplicates(subset=['account_id', 'date', 'campaign_name'])
    after_dedup = len(df_prepared)

    if before_dedup != after_dedup:
        logger.info(f"–£–¥–∞–ª–µ–Ω–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ: {before_dedup - after_dedup}")

    logger.info(f"–ò—Ç–æ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {len(df_prepared)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    if db_manager.save_dataframe(df_prepared):
        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É
        db_manager.optimize_table()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        summary_after = db_manager.get_data_summary()
        if summary_after:
            logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê:")
            logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {summary_after['total_records']:,}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π: {summary_after['unique_campaigns']:,}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–∫–∫–∞—É–Ω—Ç–æ–≤: {summary_after['unique_accounts']:,}")
            if summary_after['min_date'] and summary_after['max_date']:
                logger.info(f"  –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {summary_after['min_date']} - {summary_after['max_date']}")
            logger.info(f"  –í—Å–µ–≥–æ –ø–æ–∫–∞–∑–æ–≤: {summary_after['total_impressions']:,}")
            logger.info(f"  –í—Å–µ–≥–æ –∫–ª–∏–∫–æ–≤: {summary_after['total_clicks']:,}")
            logger.info(f"  –û–±—â–∏–µ —Ä–∞—Å—Ö–æ–¥—ã: ${summary_after['total_spend']:,.4f}")

            if summary_before:
                new_records = summary_after['total_records'] - summary_before['total_records']
                logger.info(f"  üìà –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {new_records:,}")

    else:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == '__main__':
    main()